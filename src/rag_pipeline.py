import os
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI

# âœ… Paths
BASE_PATH = "C:/Users/HP/Knowledge-Assistant/data"
VECTORSTORE_PATH = os.path.join(BASE_PATH, "vectorstore")

# âœ… Load embeddings
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# âœ… Load Chroma vector DB
db = Chroma(persist_directory=VECTORSTORE_PATH, embedding_function=embeddings)

# âœ… Retriever
retriever = db.as_retriever(search_kwargs={"k": 3})

# âœ… LLM (you can switch to "gpt-4o-mini" or any supported model)
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# âœ… Build QA chain
qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

print("ğŸš€ Knowledge Assistant is ready! Ask a question (type 'exit' to quit)\n")

# âœ… Interactive Q&A loop
while True:
    query = input("â“ Ask: ")
    if query.lower() in ["exit", "quit", "q"]:
        print("ğŸ‘‹ Goodbye!")
        break
    try:
        result = qa.run(query)
        print(f"ğŸ’¡ Answer: {result}\n")
    except Exception as e:
        print(f"âš ï¸ Error: {e}\n")
